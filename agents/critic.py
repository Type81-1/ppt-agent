import json
import os
from typing import Dict, List

from src.agents.base_agent import BaseAgent
from src.utils.llm_client import LLMClient


CRITIC_SYSTEM_PROMPT = """
You are a professional Presentation Critic. Your goal is to review slides generated by an automated system and provide constructive, actionable feedback to improve their visual quality and readability.
You may receive images of slides or the slide markdown. You must analyze them for:
1. Content Density (too much text, long bullets, overcrowding)
2. Layout (alignment, spacing, balance, overlapping)
3. Typography (font size, readability, hierarchy)
4. Visual Hierarchy (title prominence, emphasis, structure)
5. Consistency (style consistency across slides)
6. Duplicates (repeated or near-identical slides)
7. Cover Metadata (title/author/date/source separated into distinct lines or bullets)
8. Color/Contrast (readability on background)

Return your feedback in JSON using this exact structure:
{
  "feedback": [
    {
      "page_index": 1,
      "severity": "CRITICAL",
      "category": "Typography",
      "issue": "Font size too small",
      "position": "Body text",
      "evidence": "Body text looks compressed; hard to read at normal zoom.",
      "suggestion": "Increase body font size or reduce bullet count."
    }
  ],
  "summary": {
    "overall_quality": "functional | solid | professional",
    "strengths": ["..."],
    "next_focus": ["..."],
    "improvement_trend": "better | same | worse"
  }
}
If a slide is perfect, omit it from the feedback list. Always include a summary section.
Prioritize critical issues like duplicates, dense slides, tiny fonts, and misalignment.
""".strip()


class CriticAgent(BaseAgent):
    def __init__(self, model_name: str = "gpt-5.1", provider: str | None = None):
        provider = provider or os.getenv("CRITIC_LLM_PROVIDER") or "moonshot"
        super().__init__(role="Critic", model_name=model_name, provider=provider)
        self.set_system_prompt(CRITIC_SYSTEM_PROMPT)

    @staticmethod
    def _normalize_payload(payload: object) -> Dict:
        if isinstance(payload, dict):
            if "feedback" in payload:
                payload.setdefault("summary", {})
                return payload
            if "issues" in payload:
                return {"feedback": payload.get("issues", []), "summary": payload.get("summary", {})}
            return {"feedback": [], "summary": payload}
        if isinstance(payload, list):
            return {"feedback": payload, "summary": {}}
        return {"feedback": [], "summary": {}}

    def review(self, image_paths: List[str], slides_md: str | None = None) -> Dict:
        if image_paths and self.llm_client.supports_vision():
            prompt = "Review the slides and provide feedback in the required JSON format."
            response = self.chat(prompt, image_paths=image_paths, json_mode=True)
        else:
            prompt = (
                "Review the slide markdown and provide feedback in the required JSON format. "
                "Focus on visual readability, layout risks, and typography even without images.\n\n"
            )
            if slides_md:
                prompt += f"Slides Markdown:\n{slides_md}\n"
            response = self.chat(prompt, json_mode=True)
        payload = LLMClient.safe_json_loads(response.content)
        if payload is not None:
            return self._normalize_payload(payload)
        try:
            parsed = json.loads(response.content)
            return self._normalize_payload(parsed)
        except json.JSONDecodeError:
            return {"feedback": [], "summary": {}}
